{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-02T19:32:47.805403Z","iopub.status.busy":"2023-07-02T19:32:47.804531Z","iopub.status.idle":"2023-07-02T19:33:07.913901Z","shell.execute_reply":"2023-07-02T19:33:07.912753Z","shell.execute_reply.started":"2023-07-02T19:32:47.805362Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import copy\n","import os\n","import torch\n","from PIL import Image\n","from PIL import Image, ImageDraw\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from torch.utils.data import random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn as nn\n","from torchvision import utils\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:07.91659Z","iopub.status.busy":"2023-07-02T19:33:07.916069Z","iopub.status.idle":"2023-07-02T19:33:13.433291Z","shell.execute_reply":"2023-07-02T19:33:13.431872Z","shell.execute_reply.started":"2023-07-02T19:33:07.916561Z"},"trusted":true},"outputs":[],"source":["# library which allows us to view model summary like keras/tf\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:33:13.45647Z","iopub.status.busy":"2023-07-02T19:33:13.456151Z","iopub.status.idle":"2023-07-02T19:33:13.4741Z","shell.execute_reply":"2023-07-02T19:33:13.473162Z","shell.execute_reply.started":"2023-07-02T19:33:13.456443Z"},"trusted":true},"outputs":[],"source":["from IPython.core.display import display, HTML, Javascript\n","\n","color_map = ['#FFFFFF','#FF5733']\n","\n","prompt = color_map[-1]\n","main_color = color_map[0]\n","strong_main_color = color_map[1]\n","custom_colors = [strong_main_color, main_color]\n","\n","css_file = '''\n","div #notebook {\n","background-color: white;\n","line-height: 20px;\n","}\n","\n","#notebook-container {\n","%s\n","margin-top: 2em;\n","padding-top: 2em;\n","border-top: 4px solid %s;\n","-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5);\n","    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5);\n","}\n","\n","div .input {\n","margin-bottom: 1em;\n","}\n","\n",".rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\n","color: %s;\n","font-weight: 600;\n","}\n","\n","div.input_area {\n","border: none;\n","    background-color: %s;\n","    border-top: 2px solid %s;\n","}\n","\n","div.input_prompt {\n","color: %s;\n","}\n","\n","div.output_prompt {\n","color: %s; \n","}\n","\n","div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\n","background: %s;\n","}\n","\n","div.cell.selected, div.cell.selected.jupyter-soft-selected {\n","    border-color: %s;\n","}\n","\n",".edit_mode div.cell.selected:before {\n","background: %s;\n","}\n","\n",".edit_mode div.cell.selected {\n","border-color: %s;\n","\n","}\n","'''\n","\n","def to_rgb(h): \n","    return tuple(int(h[i:i+2], 16) for i in [0, 2, 4])\n","\n","main_color_rgba = 'rgba(%s, %s, %s, 0.1)' % (to_rgb(main_color[1:]))\n","open('notebook.css', 'w').write(css_file % ('width: 95%;', main_color, main_color, main_color_rgba, \n","                                            main_color,  main_color, prompt, main_color, main_color, \n","                                            main_color, main_color))\n","\n","def nb(): \n","    return HTML(\"<style>\" + open(\"notebook.css\", \"r\").read() + \"</style>\")\n","nb()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:13.475759Z","iopub.status.busy":"2023-07-02T19:33:13.475442Z","iopub.status.idle":"2023-07-02T19:33:13.978762Z","shell.execute_reply":"2023-07-02T19:33:13.977753Z","shell.execute_reply.started":"2023-07-02T19:33:13.475732Z"},"trusted":true},"outputs":[],"source":["labels_df = pd.read_csv('/histopathologic-cancer-detection/train_labels.csv')\n","print(labels_df.head().to_markdown())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:13.980575Z","iopub.status.busy":"2023-07-02T19:33:13.980257Z","iopub.status.idle":"2023-07-02T19:33:13.987093Z","shell.execute_reply":"2023-07-02T19:33:13.98616Z","shell.execute_reply.started":"2023-07-02T19:33:13.980548Z"},"trusted":true},"outputs":[],"source":["os.listdir('/histopathologic-cancer-detection/')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:13.988523Z","iopub.status.busy":"2023-07-02T19:33:13.988245Z","iopub.status.idle":"2023-07-02T19:33:13.998656Z","shell.execute_reply":"2023-07-02T19:33:13.997761Z","shell.execute_reply.started":"2023-07-02T19:33:13.988498Z"},"trusted":true},"outputs":[],"source":["labels_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:14.000186Z","iopub.status.busy":"2023-07-02T19:33:13.999896Z","iopub.status.idle":"2023-07-02T19:33:14.083418Z","shell.execute_reply":"2023-07-02T19:33:14.082496Z","shell.execute_reply.started":"2023-07-02T19:33:14.000162Z"},"trusted":true},"outputs":[],"source":["# No duplicate ids found\n","labels_df[labels_df.duplicated(keep=False)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:14.085034Z","iopub.status.busy":"2023-07-02T19:33:14.084665Z","iopub.status.idle":"2023-07-02T19:33:14.09484Z","shell.execute_reply":"2023-07-02T19:33:14.09393Z","shell.execute_reply.started":"2023-07-02T19:33:14.085003Z"},"trusted":true},"outputs":[],"source":["labels_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:33:14.099142Z","iopub.status.busy":"2023-07-02T19:33:14.098776Z","iopub.status.idle":"2023-07-02T19:33:14.119218Z","shell.execute_reply":"2023-07-02T19:33:14.118178Z","shell.execute_reply.started":"2023-07-02T19:33:14.099104Z"},"trusted":true},"outputs":[],"source":["imgpath =\"/histopathologic-cancer-detection/train/\" # training data is stored in this folder\n","malignant = labels_df.loc[labels_df['label']==1]['id'].values    # get the ids of malignant cases\n","normal = labels_df.loc[labels_df['label']==0]['id'].values       # get the ids of the normal cases\n","\n","print('normal ids')\n","print(normal[0:3],'\\n')\n","\n","print('malignant ids')\n","print(malignant[0:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:33:14.120665Z","iopub.status.busy":"2023-07-02T19:33:14.120374Z","iopub.status.idle":"2023-07-02T19:33:14.129561Z","shell.execute_reply":"2023-07-02T19:33:14.128727Z","shell.execute_reply.started":"2023-07-02T19:33:14.12064Z"},"trusted":true},"outputs":[],"source":["def plot_fig(ids,title,nrows=5,ncols=15):\n","\n","    fig,ax = plt.subplots(nrows,ncols,figsize=(18,6))\n","    plt.subplots_adjust(wspace=0, hspace=0) \n","    for i,j in enumerate(ids[:nrows*ncols]):\n","        fname = os.path.join(imgpath ,j +'.tif')\n","        img = Image.open(fname)\n","        idcol = ImageDraw.Draw(img)\n","        idcol.rectangle(((0,0),(95,95)),outline='white')\n","        plt.subplot(nrows, ncols, i+1) \n","        plt.imshow(np.array(img))\n","        plt.axis('off')\n","\n","    plt.suptitle(title, y=0.94)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:14.130937Z","iopub.status.busy":"2023-07-02T19:33:14.130661Z","iopub.status.idle":"2023-07-02T19:33:18.053492Z","shell.execute_reply":"2023-07-02T19:33:18.052443Z","shell.execute_reply.started":"2023-07-02T19:33:14.130914Z"},"trusted":true},"outputs":[],"source":["plot_fig(malignant,'Malignant Cases')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:33:18.056562Z","iopub.status.busy":"2023-07-02T19:33:18.056218Z","iopub.status.idle":"2023-07-02T19:33:21.557268Z","shell.execute_reply":"2023-07-02T19:33:21.555173Z","shell.execute_reply.started":"2023-07-02T19:33:18.056531Z"},"trusted":true},"outputs":[],"source":["plot_fig(normal,'Non-Malignant Cases')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:21.559036Z","iopub.status.busy":"2023-07-02T19:33:21.55871Z","iopub.status.idle":"2023-07-02T19:33:21.573767Z","shell.execute_reply":"2023-07-02T19:33:21.572873Z","shell.execute_reply.started":"2023-07-02T19:33:21.559008Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0) # fix random seed\n","\n","class pytorch_data(Dataset):\n","    \n","    def __init__(self,data_dir,transform,data_type=\"train\"):      \n","    \n","        # Get Image File Names\n","        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n","        \n","        file_names = os.listdir(cdm_data) # get list of images in that directory  \n","        idx_choose = np.random.choice(np.arange(len(file_names)), \n","                                      4000,\n","                                      replace=False).tolist()\n","        file_names_sample = [file_names[x] for x in idx_choose]\n","        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n","        \n","        # Get Labels\n","        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n","        labels_df=pd.read_csv(labels_data)\n","        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n","        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n","        self.transform = transform\n","      \n","    def __len__(self):\n","        return len(self.full_filenames) # size of dataset\n","      \n","    def __getitem__(self, idx):\n","        # open image, apply transforms and return with label\n","        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n","        image = self.transform(image) # Apply Specific Transformation to Image\n","        return image, self.labels[idx]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:21.575206Z","iopub.status.busy":"2023-07-02T19:33:21.574919Z","iopub.status.idle":"2023-07-02T19:33:21.587773Z","shell.execute_reply":"2023-07-02T19:33:21.586802Z","shell.execute_reply.started":"2023-07-02T19:33:21.575182Z"},"trusted":true},"outputs":[],"source":["# define transformation that converts a PIL image into PyTorch tensors\n","import torchvision.transforms as transforms\n","data_transformer = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Resize((46,46))])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:21.589493Z","iopub.status.busy":"2023-07-02T19:33:21.589145Z","iopub.status.idle":"2023-07-02T19:33:25.639712Z","shell.execute_reply":"2023-07-02T19:33:25.63862Z","shell.execute_reply.started":"2023-07-02T19:33:21.589467Z"},"trusted":true},"outputs":[],"source":["# Define an object of the custom dataset for the train folder.\n","data_dir = '/histopathologic-cancer-detection/'\n","img_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:33:25.64139Z","iopub.status.busy":"2023-07-02T19:33:25.641063Z","iopub.status.idle":"2023-07-02T19:33:25.665235Z","shell.execute_reply":"2023-07-02T19:33:25.66434Z","shell.execute_reply.started":"2023-07-02T19:33:25.641362Z"},"trusted":true},"outputs":[],"source":["# load an example tensor\n","img,label=img_dataset[10]\n","print(img.shape,torch.min(img),torch.max(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:05.85932Z","iopub.status.busy":"2023-07-02T19:34:05.857787Z","iopub.status.idle":"2023-07-02T19:34:05.868158Z","shell.execute_reply":"2023-07-02T19:34:05.866814Z","shell.execute_reply.started":"2023-07-02T19:34:05.859262Z"},"trusted":true},"outputs":[],"source":["len_img=len(img_dataset)\n","len_train=int(0.8*len_img)\n","len_val=len_img-len_train\n","\n","# Split Pytorch tensor\n","train_ts,val_ts=random_split(img_dataset,\n","                             [len_train,len_val]) # random split 80/20\n","\n","print(\"train dataset size:\", len(train_ts))\n","print(\"validation dataset size:\", len(val_ts))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:07.504092Z","iopub.status.busy":"2023-07-02T19:34:07.503218Z","iopub.status.idle":"2023-07-02T19:34:07.581352Z","shell.execute_reply":"2023-07-02T19:34:07.580424Z","shell.execute_reply.started":"2023-07-02T19:34:07.504056Z"},"trusted":true},"outputs":[],"source":["# getting the torch tensor image & target variable\n","ii=-1\n","for x,y in train_ts:\n","    print(x.shape,y)\n","    ii+=1\n","    if(ii>5):\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:34:08.732059Z","iopub.status.busy":"2023-07-02T19:34:08.731633Z","iopub.status.idle":"2023-07-02T19:34:08.770196Z","shell.execute_reply":"2023-07-02T19:34:08.768871Z","shell.execute_reply.started":"2023-07-02T19:34:08.732027Z"},"trusted":true},"outputs":[],"source":["import plotly.express as px\n","\n","def plot_img(x,y,title=None):\n","\n","    npimg = x.numpy() # convert tensor to numpy array\n","    npimg_tr=np.transpose(npimg, (1,2,0)) # Convert to H*W*C shape\n","    fig = px.imshow(npimg_tr)\n","    fig.update_layout(template='plotly_white')\n","    fig.update_layout(title=title,height=300,margin={'l':10,'r':20,'b':10})\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:11.255409Z","iopub.status.busy":"2023-07-02T19:34:11.254906Z","iopub.status.idle":"2023-07-02T19:34:11.685516Z","shell.execute_reply":"2023-07-02T19:34:11.68436Z","shell.execute_reply.started":"2023-07-02T19:34:11.255374Z"},"trusted":true},"outputs":[],"source":["# Create grid of sample images \n","grid_size=30\n","rnd_inds=np.random.randint(0,len(train_ts),grid_size)\n","print(\"image indices:\",rnd_inds)\n","\n","x_grid_train=[train_ts[i][0] for i in rnd_inds]\n","y_grid_train=[train_ts[i][1] for i in rnd_inds]\n","\n","x_grid_train=utils.make_grid(x_grid_train, nrow=10, padding=2)\n","print(x_grid_train.shape)\n","    \n","plot_img(x_grid_train,y_grid_train,'Training Subset Examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:11.845689Z","iopub.status.busy":"2023-07-02T19:34:11.845371Z","iopub.status.idle":"2023-07-02T19:34:12.250971Z","shell.execute_reply":"2023-07-02T19:34:12.249692Z","shell.execute_reply.started":"2023-07-02T19:34:11.845661Z"},"trusted":true},"outputs":[],"source":["grid_size=30\n","rnd_inds=np.random.randint(0,len(val_ts),grid_size)\n","print(\"image indices:\",rnd_inds)\n","x_grid_val=[val_ts[i][0] for i in range(grid_size)]\n","y_grid_val=[val_ts[i][1] for i in range(grid_size)]\n","\n","x_grid_val=utils.make_grid(x_grid_val, nrow=10, padding=2)\n","print(x_grid_val.shape)\n","\n","plot_img(x_grid_val,y_grid_val,'Validation Dataset Preview')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:15.579611Z","iopub.status.busy":"2023-07-02T19:34:15.578594Z","iopub.status.idle":"2023-07-02T19:34:15.58518Z","shell.execute_reply":"2023-07-02T19:34:15.584171Z","shell.execute_reply.started":"2023-07-02T19:34:15.57957Z"},"trusted":true},"outputs":[],"source":["# Define the following transformations for the training dataset\n","tr_transf = transforms.Compose([\n","#     transforms.Resize((40,40)),\n","    transforms.RandomHorizontalFlip(p=0.5), \n","    transforms.RandomVerticalFlip(p=0.5),  \n","    transforms.RandomRotation(45),         \n","#     transforms.RandomResizedCrop(50,scale=(0.8,1.0),ratio=(1.0,1.0)),\n","    transforms.ToTensor()])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:17.393871Z","iopub.status.busy":"2023-07-02T19:34:17.393423Z","iopub.status.idle":"2023-07-02T19:34:17.399238Z","shell.execute_reply":"2023-07-02T19:34:17.398242Z","shell.execute_reply.started":"2023-07-02T19:34:17.393839Z"},"trusted":true},"outputs":[],"source":["# For the validation dataset, we don't need any augmentation; simply convert images into tensors\n","val_transf = transforms.Compose([\n","    transforms.ToTensor()])\n","\n","# After defining the transformations, overwrite the transform functions of train_ts, val_ts\n","train_ts.transform=tr_transf\n","val_ts.transform=val_transf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:18.677184Z","iopub.status.busy":"2023-07-02T19:34:18.676765Z","iopub.status.idle":"2023-07-02T19:34:18.683755Z","shell.execute_reply":"2023-07-02T19:34:18.682924Z","shell.execute_reply.started":"2023-07-02T19:34:18.677152Z"},"trusted":true},"outputs":[],"source":["# The subset can also have transform attribute (if we asign)\n","train_ts.transform"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:20.161033Z","iopub.status.busy":"2023-07-02T19:34:20.160472Z","iopub.status.idle":"2023-07-02T19:34:20.167443Z","shell.execute_reply":"2023-07-02T19:34:20.166361Z","shell.execute_reply.started":"2023-07-02T19:34:20.160901Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# Training DataLoader\n","train_dl = DataLoader(train_ts,\n","                      batch_size=32, \n","                      shuffle=True)\n","\n","# Validation DataLoader\n","val_dl = DataLoader(val_ts,\n","                    batch_size=32,\n","                    shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:25.399231Z","iopub.status.busy":"2023-07-02T19:34:25.39874Z","iopub.status.idle":"2023-07-02T19:34:25.76125Z","shell.execute_reply":"2023-07-02T19:34:25.760045Z","shell.execute_reply.started":"2023-07-02T19:34:25.399197Z"},"trusted":true},"outputs":[],"source":["# check samples\n","for x,y in train_dl:\n","    print(x.shape,y)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-07-02T19:34:32.580959Z","iopub.status.busy":"2023-07-02T19:34:32.579952Z","iopub.status.idle":"2023-07-02T19:34:32.600212Z","shell.execute_reply":"2023-07-02T19:34:32.599183Z","shell.execute_reply.started":"2023-07-02T19:34:32.580916Z"},"trusted":true},"outputs":[],"source":["def findConv2dOutShape(hin,win,conv,pool=2):\n","    # get conv arguments\n","    kernel_size=conv.kernel_size\n","    stride=conv.stride\n","    padding=conv.padding\n","    dilation=conv.dilation\n","\n","    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n","    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n","\n","    if pool:\n","        hout/=pool\n","        wout/=pool\n","    return int(hout),int(wout)\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Neural Network\n","class Network(nn.Module):\n","    \n","    # Network Initialisation\n","    def __init__(self, params):\n","        \n","        super(Network, self).__init__()\n","    \n","        Cin,Hin,Win=params[\"shape_in\"]\n","        init_f=params[\"initial_filters\"] \n","        num_fc1=params[\"num_fc1\"]  \n","        num_classes=params[\"num_classes\"] \n","        self.dropout_rate=params[\"dropout_rate\"] \n","        \n","        # Convolution Layers\n","        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n","        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv2)\n","        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv3)\n","        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n","        h,w=findConv2dOutShape(h,w,self.conv4)\n","        \n","        # compute the flatten size\n","        self.num_flatten=h*w*8*init_f\n","        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n","        self.fc2 = nn.Linear(num_fc1, num_classes)\n","\n","    def forward(self,X):\n","        \n","        # Convolution & Pool Layers\n","        X = F.relu(self.conv1(X)); \n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv2(X))\n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv3(X))\n","        X = F.max_pool2d(X, 2, 2)\n","        X = F.relu(self.conv4(X))\n","        X = F.max_pool2d(X, 2, 2)\n","\n","        X = X.view(-1, self.num_flatten)\n","        \n","        X = F.relu(self.fc1(X))\n","        X=F.dropout(X, self.dropout_rate)\n","        X = self.fc2(X)\n","        return F.log_softmax(X, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:32.845491Z","iopub.status.busy":"2023-07-02T19:34:32.845167Z","iopub.status.idle":"2023-07-02T19:34:32.854694Z","shell.execute_reply":"2023-07-02T19:34:32.853661Z","shell.execute_reply.started":"2023-07-02T19:34:32.845464Z"},"trusted":true},"outputs":[],"source":["# Neural Network Predefined Parameters\n","params_model={\n","        \"shape_in\": (3,46,46), \n","        \"initial_filters\": 8,    \n","        \"num_fc1\": 100,\n","        \"dropout_rate\": 0.25,\n","        \"num_classes\": 2}\n","\n","# Create instantiation of Network class\n","cnn_model = Network(params_model)\n","\n","# define computation hardware approach (GPU/CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = cnn_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:34.112759Z","iopub.status.busy":"2023-07-02T19:34:34.112307Z","iopub.status.idle":"2023-07-02T19:34:34.136433Z","shell.execute_reply":"2023-07-02T19:34:34.135072Z","shell.execute_reply.started":"2023-07-02T19:34:34.112729Z"},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","summary(cnn_model, input_size=(3, 46, 46),device=device.type)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:37.616438Z","iopub.status.busy":"2023-07-02T19:34:37.615984Z","iopub.status.idle":"2023-07-02T19:34:37.621585Z","shell.execute_reply":"2023-07-02T19:34:37.620385Z","shell.execute_reply.started":"2023-07-02T19:34:37.616404Z"},"trusted":true},"outputs":[],"source":["loss_func = nn.NLLLoss(reduction=\"sum\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:39.627791Z","iopub.status.busy":"2023-07-02T19:34:39.627355Z","iopub.status.idle":"2023-07-02T19:34:39.63405Z","shell.execute_reply":"2023-07-02T19:34:39.633024Z","shell.execute_reply.started":"2023-07-02T19:34:39.627759Z"},"trusted":true},"outputs":[],"source":["from torch import optim\n","opt = optim.Adam(cnn_model.parameters(), lr=3e-4)\n","lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:34:44.603481Z","iopub.status.busy":"2023-07-02T19:34:44.602224Z","iopub.status.idle":"2023-07-02T19:34:44.614585Z","shell.execute_reply":"2023-07-02T19:34:44.613513Z","shell.execute_reply.started":"2023-07-02T19:34:44.60344Z"},"trusted":true},"outputs":[],"source":["''' Helper Functions'''\n","\n","# Function to get the learning rate\n","def get_lr(opt):\n","    for param_group in opt.param_groups:\n","        return param_group['lr']\n","\n","# Function to compute the loss value per batch of data\n","def loss_batch(loss_func, output, target, opt=None):\n","    \n","    loss = loss_func(output, target) # get loss\n","    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n","    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n","    \n","    if opt is not None:\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    return loss.item(), metric_b\n","\n","# Compute the loss value & performance metric for the entire dataset (epoch)\n","def loss_epoch(model,loss_func,dataset_dl,opt=None):\n","    \n","    run_loss=0.0 \n","    t_metric=0.0\n","    len_data=len(dataset_dl.dataset)\n","\n","    # internal loop over dataset\n","    for xb, yb in dataset_dl:\n","        # move batch to device\n","        xb=xb.to(device)\n","        yb=yb.to(device)\n","        output=model(xb) # get model output\n","        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n","        run_loss+=loss_b        # update running loss\n","\n","        if metric_b is not None: # update running metric\n","            t_metric+=metric_b    \n","    \n","    loss=run_loss/float(len_data)  # average loss value\n","    metric=t_metric/float(len_data) # average metric value\n","    \n","    return loss, metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:34:46.787801Z","iopub.status.busy":"2023-07-02T19:34:46.787371Z","iopub.status.idle":"2023-07-02T19:34:46.794696Z","shell.execute_reply":"2023-07-02T19:34:46.793767Z","shell.execute_reply.started":"2023-07-02T19:34:46.787769Z"},"trusted":true},"outputs":[],"source":["params_train={\n"," \"train\": train_dl,\"val\": val_dl,\n"," \"epochs\": 50,\n"," \"optimiser\": optim.Adam(cnn_model.parameters(),\n","                         lr=3e-4),\n"," \"lr_change\": ReduceLROnPlateau(opt,\n","                                mode='min',\n","                                factor=0.5,\n","                                patience=20,\n","                                verbose=0),\n"," \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n"," \"weight_path\": \"weights.pt\",\n"," \"check\": False, \n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:39:49.729116Z","iopub.status.busy":"2023-07-02T19:39:49.728671Z","iopub.status.idle":"2023-07-02T19:39:49.744789Z","shell.execute_reply":"2023-07-02T19:39:49.743783Z","shell.execute_reply.started":"2023-07-02T19:39:49.729084Z"},"trusted":true},"outputs":[],"source":["from tqdm.notebook import trange, tqdm\n","\n","def train_val(model, params,verbose=False):\n","    \n","    # Get the parameters\n","    epochs=params[\"epochs\"]\n","    loss_func=params[\"f_loss\"]\n","    opt=params[\"optimiser\"]\n","    train_dl=params[\"train\"]\n","    val_dl=params[\"val\"]\n","    lr_scheduler=params[\"lr_change\"]\n","    weight_path=params[\"weight_path\"]\n","    \n","    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n","    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n","    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n","    best_loss=float('inf') # initialize best loss to a large value\n","    \n","    ''' Train Model n_epochs '''\n","    \n","    for epoch in tqdm(range(epochs), desc=\"Training\"):\n","        \n","        ''' Get the Learning Rate '''\n","        current_lr=get_lr(opt)\n","        if(verbose):\n","            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n","        \n","        '''\n","        \n","        Train Model Process\n","        \n","        '''\n","        \n","        model.train()\n","        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n","\n","        # collect losses\n","        loss_history[\"train\"].append(train_loss)\n","        metric_history[\"train\"].append(train_metric)\n","        \n","        '''\n","        \n","        Evaluate Model Process\n","        \n","        '''\n","        \n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n","        \n","        # store best model\n","        if(val_loss < best_loss):\n","            best_loss = val_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            \n","            # store weights into a local file\n","            torch.save(model.state_dict(), weight_path)\n","            if(verbose):\n","                print(\"Copied best model weights!\")\n","        \n","        # collect loss and metric for validation dataset\n","        loss_history[\"val\"].append(val_loss)\n","        metric_history[\"val\"].append(val_metric)\n","        \n","        # learning rate schedule\n","        lr_scheduler.step(val_loss)\n","        if current_lr != get_lr(opt):\n","            if(verbose):\n","                print(\"Loading best model weights!\")\n","            model.load_state_dict(best_model_wts) \n","\n","        if(verbose):\n","            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n","            print(\"-\"*10) \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","        \n","    return model, loss_history, metric_history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:39:50.011172Z","iopub.status.busy":"2023-07-02T19:39:50.010766Z","iopub.status.idle":"2023-07-02T19:39:50.127722Z","shell.execute_reply":"2023-07-02T19:39:50.126547Z","shell.execute_reply.started":"2023-07-02T19:39:50.011141Z"},"trusted":true},"outputs":[],"source":["params_train={\n"," \"train\": train_dl,\"val\": val_dl,\n"," \"epochs\": 50,\n"," \"optimiser\": optim.Adam(cnn_model.parameters(),lr=3e-4),\n"," \"lr_change\": ReduceLROnPlateau(opt,\n","                                mode='min',\n","                                factor=0.5,\n","                                patience=20,\n","                                verbose=0),\n"," \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n"," \"weight_path\": \"weights.pt\",\n","}\n","\n","''' Actual Train / Evaluation of CNN Model '''\n","# train and validate the model\n","\n","cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-02T19:39:50.663165Z","iopub.status.busy":"2023-07-02T19:39:50.662765Z","iopub.status.idle":"2023-07-02T19:39:50.701987Z","shell.execute_reply":"2023-07-02T19:39:50.700791Z","shell.execute_reply.started":"2023-07-02T19:39:50.663115Z"},"trusted":true},"outputs":[],"source":["import seaborn as sns; sns.set(style='whitegrid')\n","\n","epochs=params_train[\"epochs\"]\n","\n","fig,ax = plt.subplots(1,2,figsize=(12,5))\n","\n","sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n","sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n","sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='metric_hist[\"train\"]')\n","sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='metric_hist[\"val\"]')\n","plt.title('Convergence History')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-07-02T19:39:15.766455Z","iopub.status.busy":"2023-07-02T19:39:15.765577Z","iopub.status.idle":"2023-07-02T19:39:15.771104Z","shell.execute_reply":"2023-07-02T19:39:15.77007Z","shell.execute_reply.started":"2023-07-02T19:39:15.766409Z"},"trusted":true},"outputs":[],"source":["# # Train-Validation Progress\n","# epochs=params_train[\"epochs\"]\n","\n","# fig = make_subplots(rows=1, cols=2,subplot_titles=['lost_hist','metric_hist'])\n","# fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"train\"],name='loss_hist[\"train\"]'),row=1, col=1)\n","# fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"val\"],name='loss_hist[\"val\"]'),row=1, col=1)\n","# fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"train\"],name='metric_hist[\"train\"]'),row=1, col=2)\n","# fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"val\"],name='metric_hist[\"val\"]'),row=1, col=2)\n","# fig.update_layout(template='plotly_white');fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\n","# fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T19:39:17.138278Z","iopub.status.busy":"2023-07-02T19:39:17.137178Z","iopub.status.idle":"2023-07-02T19:39:17.148029Z","shell.execute_reply":"2023-07-02T19:39:17.147113Z","shell.execute_reply.started":"2023-07-02T19:39:17.13824Z"},"trusted":true},"outputs":[],"source":["class pytorchdata_test(Dataset):\n","    \n","    def __init__(self, data_dir, transform,data_type=\"train\"):\n","        \n","        path2data = os.path.join(data_dir,data_type)\n","        filenames = os.listdir(path2data)\n","        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n","        \n","        # labels are in a csv file named train_labels.csv\n","        csv_filename=\"sample_submission.csv\"\n","        path2csvLabels=os.path.join(data_dir,csv_filename)\n","        labels_df=pd.read_csv(path2csvLabels)\n","        \n","        # set data frame index to id\n","        labels_df.set_index(\"id\", inplace=True)\n","        \n","        # obtain labels from data frame\n","        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n","        self.transform = transform       \n","        \n","    def __len__(self):\n","        # return size of dataset\n","        return len(self.full_filenames)\n","    \n","    def __getitem__(self, idx):\n","        # open image, apply transforms and return with label\n","        image = Image.open(self.full_filenames[idx]) # PIL image\n","        image = self.transform(image)\n","        return image, self.labels[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.988767Z","iopub.status.idle":"2023-07-02T19:33:25.989133Z","shell.execute_reply":"2023-07-02T19:33:25.988958Z","shell.execute_reply.started":"2023-07-02T19:33:25.988941Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.990375Z","iopub.status.idle":"2023-07-02T19:33:25.99071Z","shell.execute_reply":"2023-07-02T19:33:25.990555Z","shell.execute_reply.started":"2023-07-02T19:33:25.990539Z"},"trusted":true},"outputs":[],"source":["!ls '/histopathologic-cancer-detection/test' | head -n 5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.992038Z","iopub.status.idle":"2023-07-02T19:33:25.992397Z","shell.execute_reply":"2023-07-02T19:33:25.992238Z","shell.execute_reply.started":"2023-07-02T19:33:25.992222Z"},"trusted":true},"outputs":[],"source":["# load any model weights for the model\n","cnn_model.load_state_dict(torch.load('weights.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.993458Z","iopub.status.idle":"2023-07-02T19:33:25.993792Z","shell.execute_reply":"2023-07-02T19:33:25.993636Z","shell.execute_reply.started":"2023-07-02T19:33:25.99362Z"},"trusted":true},"outputs":[],"source":["# sample submission\n","path_sub = \"/histopathologic-cancer-detection/sample_submission.csv\"\n","labels_df = pd.read_csv(path_sub)\n","labels_df.head()\n","labels_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.994719Z","iopub.status.idle":"2023-07-02T19:33:25.995057Z","shell.execute_reply":"2023-07-02T19:33:25.994904Z","shell.execute_reply.started":"2023-07-02T19:33:25.994889Z"},"trusted":true},"outputs":[],"source":["data_dir = '/histopathologic-cancer-detection/'\n","\n","data_transformer = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Resize((46,46))])\n","\n","img_dataset_test = pytorchdata_test(data_dir,data_transformer,data_type=\"test\")\n","print(len(img_dataset_test), 'samples found')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.996236Z","iopub.status.idle":"2023-07-02T19:33:25.996634Z","shell.execute_reply":"2023-07-02T19:33:25.996446Z","shell.execute_reply.started":"2023-07-02T19:33:25.996428Z"},"trusted":true},"outputs":[],"source":["def inference(model,dataset,device,num_classes=2):\n","    \n","    len_data=len(dataset)\n","    y_out=torch.zeros(len_data,num_classes) # initialize output tensor on CPU\n","    y_gt=np.zeros((len_data),dtype=\"uint8\") # initialize ground truth on CPU\n","    model=model.to(device) # move model to device\n","    \n","    with torch.no_grad():\n","        for i in tqdm(range(len_data)):\n","            x,y=dataset[i]\n","            y_gt[i]=y\n","            y_out[i]=model(x.unsqueeze(0).to(device))\n","\n","    return y_out.numpy(),y_gt            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.998319Z","iopub.status.idle":"2023-07-02T19:33:25.998666Z","shell.execute_reply":"2023-07-02T19:33:25.998503Z","shell.execute_reply.started":"2023-07-02T19:33:25.998487Z"},"trusted":true},"outputs":[],"source":["y_test_out,_ = inference(cnn_model,img_dataset_test, device)            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:25.999935Z","iopub.status.idle":"2023-07-02T19:33:26.0003Z","shell.execute_reply":"2023-07-02T19:33:26.000138Z","shell.execute_reply.started":"2023-07-02T19:33:26.000109Z"},"trusted":true},"outputs":[],"source":["# class predictions 0,1\n","y_test_pred=np.argmax(y_test_out,axis=1)\n","print(y_test_pred.shape)\n","print(y_test_pred[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-02T19:33:26.001393Z","iopub.status.idle":"2023-07-02T19:33:26.001727Z","shell.execute_reply":"2023-07-02T19:33:26.001572Z","shell.execute_reply.started":"2023-07-02T19:33:26.001556Z"},"trusted":true},"outputs":[],"source":["# probabilities of predicted selection\n","# return F.log_softmax(x, dim=1) ie.\n","preds = np.exp(y_test_out[:, 1])\n","print(preds.shape)\n","print(preds[0:5])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
